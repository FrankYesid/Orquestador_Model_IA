# üöÄ Proyecto de Orquestaci√≥n ETL con Apache Airflow

![Python](https://img.shields.io/badge/Python-3.11-blue.svg)
![Airflow](https://img.shields.io/badge/Airflow-2.8.1-orange.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Educational-yellow.svg)

## üìã Descripci√≥n

Proyecto educativo completo que demuestra el uso de **Apache Airflow** como orquestador de flujos de datos ETL (Extract, Transform, Load). Este proyecto implementa un pipeline de procesamiento de datos de ventas desde la extracci√≥n hasta la carga en una base de datos, mostrando las mejores pr√°cticas en orquestaci√≥n de datos.

### üéØ Objetivos del Proyecto

- ‚úÖ Demostrar el uso pr√°ctico de Apache Airflow
- ‚úÖ Implementar un flujo ETL completo y funcional
- ‚úÖ Mostrar buenas pr√°cticas en organizaci√≥n de c√≥digo
- ‚úÖ Proporcionar documentaci√≥n detallada y educativa
- ‚úÖ Incluir configuraci√≥n con Docker para f√°cil despliegue

---

## üèóÔ∏è Arquitectura del Proyecto

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     APACHE AIRFLOW                          ‚îÇ
‚îÇ                     (Orquestador)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚ñº                   ‚ñº                   ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ EXTRACT ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇTRANSFORM‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  LOAD   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚ñº                   ‚ñº                   ‚ñº
   SQLite DB          Pandas DF           SQLite DB
   (Origen)          (Limpieza)          (Destino)
```

### Flujo de Datos

1. **Extract**: Extrae datos desde la base de datos SQLite de origen
2. **Transform**: Limpia, valida y transforma los datos usando pandas
3. **Load**: Carga los datos procesados en la tabla destino

---

## üìÅ Estructura del Proyecto

```
airflow_project/
‚îú‚îÄ‚îÄ üìÇ dags/                          # DAGs de Airflow
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ etl_pipeline.py              # DAG principal del pipeline ETL
‚îÇ
‚îú‚îÄ‚îÄ üìÇ scripts/                       # Scripts modulares ETL
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ create_dummy_db.py           # Crea base de datos con datos dummy
‚îÇ   ‚îú‚îÄ‚îÄ extract_data.py              # Script de extracci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ transform_data.py            # Script de transformaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ load_data.py                 # Script de carga
‚îÇ
‚îú‚îÄ‚îÄ üìÇ config/                        # Configuraci√≥n del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ db_config.py                 # Configuraci√≥n de base de datos
‚îÇ
‚îú‚îÄ‚îÄ üìÇ data/                          # Datos del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ input/                       # Datos de entrada
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ .gitkeep
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dummy_data.csv          # Datos de ejemplo (generado)
‚îÇ   ‚îú‚îÄ‚îÄ output/                      # Datos procesados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ .gitkeep
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extracted_data.csv      # Datos extra√≠dos (generado)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformed_data.csv    # Datos transformados (generado)
‚îÇ   ‚îî‚îÄ‚îÄ database.db                  # Base de datos SQLite (generado)
‚îÇ
‚îú‚îÄ‚îÄ üìÇ docker/                        # Configuraci√≥n de Docker
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                   # Imagen personalizada de Airflow
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml           # Orquestaci√≥n de contenedores
‚îÇ   ‚îî‚îÄ‚îÄ .env.docker                  # Variables de entorno para Docker
‚îÇ
‚îú‚îÄ‚îÄ üìÇ .github/                       # GitHub Actions
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ ci.yml                   # Pipeline de CI/CD
‚îÇ
‚îú‚îÄ‚îÄ üìÇ airflow/                       # Directorio de Airflow (generado)
‚îÇ   ‚îú‚îÄ‚îÄ logs/                        # Logs de ejecuci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ airflow.db                   # Base de datos de metadatos
‚îÇ   ‚îî‚îÄ‚îÄ airflow.cfg                  # Configuraci√≥n de Airflow
‚îÇ
‚îú‚îÄ‚îÄ üìÑ requirements.txt               # Dependencias de Python
‚îú‚îÄ‚îÄ üìÑ .env                          # Variables de entorno (local)
‚îú‚îÄ‚îÄ üìÑ .env.example                  # Ejemplo de variables de entorno
‚îú‚îÄ‚îÄ üìÑ .gitignore                    # Archivos ignorados por Git
‚îú‚îÄ‚îÄ üìÑ LICENSE                       # Licencia MIT
‚îî‚îÄ‚îÄ üìÑ README.md                     # Este archivo
```

---

## üõ†Ô∏è Tecnolog√≠as Utilizadas

| Tecnolog√≠a | Versi√≥n | Prop√≥sito |
|------------|---------|-----------|
| **Python** | 3.11 | Lenguaje principal |
| **Apache Airflow** | 2.8.1 | Orquestador de flujos |
| **Pandas** | 2.1.4 | Procesamiento de datos |
| **SQLite** | 3.x | Base de datos |
| **Docker** | Latest | Contenedorizaci√≥n |
| **Docker Compose** | Latest | Orquestaci√≥n de contenedores |

---

## üöÄ Instalaci√≥n y Configuraci√≥n

### Opci√≥n 1: Instalaci√≥n Local (Sin Docker)

#### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/tu-usuario/Orquestador_Model_IA.git
cd Orquestador_Model_IA
```

#### Paso 2: Crear Entorno Virtual

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

#### Paso 3: Instalar Dependencias

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

#### Paso 4: Configurar Variables de Entorno

```bash
# Copiar el archivo de ejemplo
cp .env.example .env

# Editar .env con las rutas correctas de tu sistema
# En Windows, usa rutas absolutas como: D:/GitHub/Orquestador_Model_IA
```

#### Paso 5: Inicializar Base de Datos Dummy

```bash
python scripts/create_dummy_db.py
```

**Salida esperada:**
```
============================================================
INICIANDO CREACI√ìN DE BASE DE DATOS DUMMY
============================================================
Generando 1000 registros de datos dummy...
Datos generados exitosamente: (1000, 11)
Creando tablas en la base de datos...
Tabla 'sales_data' creada exitosamente
Tabla 'sales_transformed' creada exitosamente
Insertando datos en la base de datos...
1000 registros insertados en 'sales_data'
============================================================
‚úì Base de datos creada exitosamente
‚úì Total de registros en BD: 1000
‚úì Ubicaci√≥n: d:/GitHub/Orquestador_Model_IA/data/database.db
============================================================
```

#### Paso 6: Configurar Airflow

```bash
# Establecer AIRFLOW_HOME
export AIRFLOW_HOME=$(pwd)/airflow  # Linux/Mac
set AIRFLOW_HOME=%cd%\airflow       # Windows CMD
$env:AIRFLOW_HOME="$PWD\airflow"    # Windows PowerShell

# Inicializar base de datos de Airflow
airflow db init

# Crear usuario administrador
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin
```

#### Paso 7: Iniciar Airflow

```bash
# Terminal 1: Iniciar el Scheduler
airflow scheduler

# Terminal 2: Iniciar el Webserver
airflow webserver --port 8080
```

#### Paso 8: Acceder a la Interfaz Web

Abre tu navegador y ve a: **http://localhost:8080**

- **Usuario**: `admin`
- **Contrase√±a**: `admin`

---

### Opci√≥n 2: Instalaci√≥n con Docker üê≥

#### Paso 1: Requisitos Previos

- Docker Desktop instalado
- Docker Compose instalado

#### Paso 2: Clonar el Repositorio

```bash
git clone https://github.com/tu-usuario/Orquestador_Model_IA.git
cd Orquestador_Model_IA
```

#### Paso 3: Configurar Variables de Entorno

```bash
cd docker
cp .env.docker .env
```

#### Paso 4: Inicializar Base de Datos Dummy (Opcional)

```bash
# Volver al directorio ra√≠z
cd ..

# Crear entorno virtual temporal
python -m venv temp_venv
temp_venv\Scripts\activate  # Windows
# source temp_venv/bin/activate  # Linux/Mac

# Instalar dependencias m√≠nimas
pip install pandas python-dotenv

# Crear base de datos
python scripts/create_dummy_db.py

# Desactivar entorno
deactivate
```

#### Paso 5: Iniciar Servicios con Docker Compose

```bash
cd docker
docker-compose up -d
```

**Servicios que se iniciar√°n:**
- `postgres`: Base de datos para metadatos de Airflow
- `airflow-init`: Inicializaci√≥n de Airflow
- `airflow-webserver`: Interfaz web (puerto 8080)
- `airflow-scheduler`: Programador de tareas

#### Paso 6: Verificar Estado de los Contenedores

```bash
docker-compose ps
```

#### Paso 7: Acceder a la Interfaz Web

Abre tu navegador y ve a: **http://localhost:8080**

- **Usuario**: `airflow`
- **Contrase√±a**: `airflow`

#### Comandos √ötiles de Docker

```bash
# Ver logs
docker-compose logs -f

# Detener servicios
docker-compose down

# Reiniciar servicios
docker-compose restart

# Eliminar todo (incluyendo vol√∫menes)
docker-compose down -v
```

---

## üìä Uso del Pipeline ETL

### 1. Activar el DAG

1. Accede a la interfaz web de Airflow
2. Busca el DAG llamado `etl_sales_pipeline`
3. Activa el toggle en la columna "Active"

### 2. Ejecutar Manualmente

1. Haz clic en el nombre del DAG
2. Haz clic en el bot√≥n "Trigger DAG" (‚ñ∂Ô∏è)
3. Confirma la ejecuci√≥n

### 3. Monitorear la Ejecuci√≥n

#### Vista de Grafo (Graph View)
```
[start_pipeline] ‚Üí [extract_data] ‚Üí [transform_data] ‚Üí [load_data] ‚Üí [generate_report] ‚Üí [end_pipeline]
```

- **Verde**: Tarea completada exitosamente
- **Rojo**: Tarea fallida
- **Amarillo**: Tarea en ejecuci√≥n
- **Gris**: Tarea pendiente

#### Ver Logs de una Tarea

1. Haz clic en la tarea en el grafo
2. Selecciona "Log"
3. Revisa la salida detallada

### 4. Verificar Resultados

#### Opci√≥n A: Desde la Base de Datos

```bash
# Conectar a SQLite
sqlite3 data/database.db

# Ver datos transformados
SELECT * FROM sales_transformed LIMIT 10;

# Ver estad√≠sticas
SELECT 
    COUNT(*) as total_records,
    SUM(total_venta) as total_sales,
    AVG(precio_promedio) as avg_price
FROM sales_transformed;

# Salir
.quit
```

#### Opci√≥n B: Desde los Archivos CSV

```bash
# Ver datos extra√≠dos
cat data/output/extracted_data.csv | head

# Ver datos transformados
cat data/output/transformed_data.csv | head
```

#### Opci√≥n C: Con Python/Pandas

```python
import pandas as pd

# Leer datos transformados
df = pd.read_csv('data/output/transformed_data.csv')

# Ver primeras filas
print(df.head())

# Ver estad√≠sticas
print(df.describe())

# Ver totales por regi√≥n
print(df.groupby('region')['total_venta'].sum())
```

---

## üîß Configuraci√≥n Avanzada

### Programaci√≥n del DAG

El DAG est√° configurado para ejecutarse diariamente a medianoche. Para cambiar la frecuencia:

```python
# En dags/etl_pipeline.py
schedule_interval='0 0 * * *',  # Cron expression

# Ejemplos:
# '0 */6 * * *'   - Cada 6 horas
# '0 0 * * 1'     - Cada lunes
# '@hourly'       - Cada hora
# '@daily'        - Diario
# None            - Solo manual
```

### Variables de Airflow

Puedes configurar variables en la UI de Airflow:

1. Admin ‚Üí Variables
2. Agregar nueva variable:
   - Key: `etl_batch_size`
   - Value: `1000`

Usar en el c√≥digo:
```python
from airflow.models import Variable

batch_size = Variable.get("etl_batch_size", default_var=500)
```

### Conexiones

Para conectar a bases de datos externas:

1. Admin ‚Üí Connections
2. Agregar nueva conexi√≥n
3. Configurar tipo, host, puerto, credenciales

---

## üìà Descripci√≥n Detallada de los Scripts

### 1. `create_dummy_db.py`

**Prop√≥sito**: Genera datos de ventas simulados para demostraci√≥n.

**Funcionalidades**:
- Genera 1000 registros de ventas con datos aleatorios
- Crea tablas `sales_data` (origen) y `sales_transformed` (destino)
- Incluye productos, categor√≠as, regiones, precios, descuentos
- Agrega valores nulos intencionalmente para demostrar limpieza

**Datos generados**:
- 10 productos diferentes
- 4 categor√≠as
- 5 regiones
- Rango de fechas: 2023
- Valores de venta: $10 - $1,500

### 2. `extract_data.py`

**Prop√≥sito**: Extrae datos desde la base de datos SQLite.

**Proceso**:
1. Conecta a la base de datos
2. Ejecuta query SELECT para extraer todos los registros
3. Valida los datos extra√≠dos:
   - Verifica que no est√© vac√≠o
   - Valida columnas requeridas
   - Convierte tipos de datos
   - Reporta valores nulos
4. Guarda datos en `extracted_data.csv`
5. Genera resumen de extracci√≥n

**Validaciones**:
- ‚úÖ DataFrame no vac√≠o
- ‚úÖ Columnas requeridas presentes
- ‚úÖ Fechas en formato correcto
- ‚úÖ Valores num√©ricos v√°lidos

### 3. `transform_data.py`

**Prop√≥sito**: Limpia, valida y transforma los datos.

**Proceso de Limpieza**:
1. **Duplicados**: Elimina registros duplicados completos
2. **Valores nulos**: 
   - Rellena descuentos nulos con 0
   - Elimina filas con nulos en columnas cr√≠ticas
3. **Validaci√≥n num√©rica**:
   - Elimina cantidades ‚â§ 0
   - Elimina precios ‚â§ 0
4. **Outliers**: Usa m√©todo IQR (Interquartile Range) para detectar y eliminar outliers
5. **Normalizaci√≥n**: Normaliza texto (capitalizaci√≥n, espacios)

**Transformaciones**:
- Extrae componentes de fecha (a√±o, mes, d√≠a, trimestre)
- Calcula m√©tricas derivadas (ingreso bruto, descuento total, margen)
- Categoriza ventas (Peque√±a, Mediana, Grande, Premium)
- Crea flags booleanos (tiene_descuento, venta_alta)

**Agregaci√≥n**:
- Agrupa por: fecha, producto, categor√≠a, regi√≥n
- Suma cantidades
- Promedia precios y descuentos
- Cuenta n√∫mero de transacciones

### 4. `load_data.py`

**Prop√≥sito**: Carga datos transformados en la base de datos destino.

**Proceso**:
1. Carga datos desde `transformed_data.csv`
2. Valida datos antes de la carga:
   - Verifica que no est√© vac√≠o
   - Valida columnas requeridas
   - Verifica valores nulos en columnas cr√≠ticas
   - Valida valores num√©ricos positivos
3. Prepara datos:
   - Convierte fechas a formato string
   - Asegura tipos de datos correctos
   - Ordena por fecha
4. Crea backup de datos existentes (opcional)
5. Inserta datos en tabla `sales_transformed`
6. Crea √≠ndices para optimizar consultas
7. Genera estad√≠sticas de carga

**√çndices creados**:
- `idx_sales_transformed_fecha`
- `idx_sales_transformed_producto`
- `idx_sales_transformed_categoria`
- `idx_sales_transformed_region`

---

## üß™ Testing

### Ejecutar Tests Unitarios

```bash
# Instalar pytest
pip install pytest pytest-cov

# Ejecutar todos los tests
pytest tests/ -v

# Con cobertura
pytest tests/ --cov=scripts --cov-report=html
```

### Probar Scripts Individualmente

```bash
# Probar extracci√≥n
python scripts/extract_data.py

# Probar transformaci√≥n
python scripts/transform_data.py

# Probar carga
python scripts/load_data.py
```

### Probar DAG sin Ejecutar

```bash
# Verificar sintaxis del DAG
python dags/etl_pipeline.py

# Listar DAGs
airflow dags list

# Probar DAG (sin ejecutar)
airflow dags test etl_sales_pipeline 2025-01-01

# Probar una tarea espec√≠fica
airflow tasks test etl_sales_pipeline extract_data 2025-01-01
```

---

## üìä M√©tricas y Monitoreo

### M√©tricas del Pipeline

El pipeline genera las siguientes m√©tricas:

1. **Extracci√≥n**:
   - Total de registros extra√≠dos
   - Rango de fechas
   - Total de ventas
   - Valores nulos encontrados

2. **Transformaci√≥n**:
   - Registros originales vs transformados
   - Porcentaje de reducci√≥n
   - Registros eliminados por limpieza
   - Outliers removidos

3. **Carga**:
   - Registros cargados
   - Total de ventas en destino
   - Top 5 productos
   - Top 5 regiones

### Logs

Los logs se encuentran en:
- **Local**: `airflow/logs/dag_id/task_id/execution_date/`
- **Docker**: Accesibles v√≠a `docker-compose logs`

### Alertas

Configurar alertas por email en caso de fallo:

```python
# En default_args del DAG
'email': ['tu-email@example.com'],
'email_on_failure': True,
'email_on_retry': True,
```

---

## üêõ Troubleshooting

### Problema: Airflow no encuentra los DAGs

**Soluci√≥n**:
```bash
# Verificar AIRFLOW_HOME
echo $AIRFLOW_HOME  # Linux/Mac
echo %AIRFLOW_HOME%  # Windows

# Verificar configuraci√≥n
airflow config get-value core dags_folder

# Listar DAGs
airflow dags list
```

### Problema: Error de importaci√≥n de m√≥dulos

**Soluci√≥n**:
```bash
# Agregar directorio al PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"  # Linux/Mac
set PYTHONPATH=%PYTHONPATH%;%cd%  # Windows
```

### Problema: Base de datos no existe

**Soluci√≥n**:
```bash
# Ejecutar script de creaci√≥n
python scripts/create_dummy_db.py

# Verificar que se cre√≥
ls -la data/database.db  # Linux/Mac
dir data\database.db  # Windows
```

### Problema: Permisos en Docker

**Soluci√≥n**:
```bash
# Linux/Mac: Usar tu UID
echo -e "AIRFLOW_UID=$(id -u)" > docker/.env

# Windows: Usar 50000
echo AIRFLOW_UID=50000 > docker/.env
```

### Problema: Puerto 8080 en uso

**Soluci√≥n**:
```bash
# Cambiar puerto en docker-compose.yml
ports:
  - "8081:8080"  # Usar 8081 en lugar de 8080

# O detener el proceso que usa el puerto
# Windows
netstat -ano | findstr :8080
taskkill /PID <PID> /F

# Linux/Mac
lsof -ti:8080 | xargs kill -9
```

---

## üîí Mejores Pr√°cticas

### Seguridad

1. **No commitear credenciales**: Usa `.env` y `.gitignore`
2. **Usar Secrets**: Para producci√≥n, usa Airflow Secrets Backend
3. **Validar inputs**: Siempre valida datos antes de procesarlos
4. **Logs seguros**: No loguear informaci√≥n sensible

### Rendimiento

1. **Batch processing**: Procesa datos en lotes
2. **√çndices**: Crea √≠ndices en columnas frecuentemente consultadas
3. **Paralelizaci√≥n**: Usa `max_active_runs` y `concurrency`
4. **Limpieza**: Elimina logs antiguos regularmente

### Mantenibilidad

1. **C√≥digo modular**: Separa l√≥gica en funciones reutilizables
2. **Documentaci√≥n**: Documenta DAGs y funciones
3. **Testing**: Escribe tests para funciones cr√≠ticas
4. **Versionado**: Usa Git para control de versiones

---

## üìö Recursos Adicionales

### Documentaci√≥n Oficial

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [SQLite Documentation](https://www.sqlite.org/docs.html)

### Tutoriales Recomendados

- [Airflow Tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html)
- [ETL Best Practices](https://www.astronomer.io/guides/)
- [Data Pipeline Design Patterns](https://www.oreilly.com/library/view/data-pipelines-pocket/9781492087823/)

### Comunidad

- [Airflow Slack](https://apache-airflow.slack.com/)
- [Stack Overflow - Airflow Tag](https://stackoverflow.com/questions/tagged/airflow)
- [Reddit - r/dataengineering](https://www.reddit.com/r/dataengineering/)

---

## ü§ù Contribuciones

Las contribuciones son bienvenidas! Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

---

## üìù Changelog

### Version 1.0.0 (2025-01-15)
- ‚ú® Implementaci√≥n inicial del proyecto
- ‚ú® Pipeline ETL completo con Airflow
- ‚ú® Scripts modulares de extracci√≥n, transformaci√≥n y carga
- ‚ú® Configuraci√≥n con Docker
- ‚ú® Documentaci√≥n completa
- ‚ú® Datos dummy para demostraci√≥n

---

## üë• Autores

- **Proyecto Educativo ETL** - *Trabajo Inicial* - [GitHub](https://github.com/tu-usuario)

---

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para m√°s detalles.

---

## üôè Agradecimientos

- Apache Airflow Community
- Pandas Development Team
- Todos los contribuidores de c√≥digo abierto

---

## üìû Contacto

¬øPreguntas o sugerencias? Abre un [issue](https://github.com/tu-usuario/Orquestador_Model_IA/issues) en GitHub.

---

<div align="center">

**‚≠ê Si este proyecto te fue √∫til, considera darle una estrella en GitHub ‚≠ê**

Made with ‚ù§Ô∏è for the Data Engineering Community

</div>
