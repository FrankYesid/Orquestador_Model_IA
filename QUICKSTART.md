# âš¡ GuÃ­a de Inicio RÃ¡pido

Esta guÃ­a te ayudarÃ¡ a poner en marcha el proyecto en **menos de 10 minutos**.

## ğŸ¯ OpciÃ³n 1: Inicio RÃ¡pido Local (Recomendado para Windows)

### Paso 1: Requisitos Previos

- âœ… Python 3.11 instalado
- âœ… Git instalado

### Paso 2: Clonar y Configurar (2 minutos)

```powershell
# Clonar el repositorio
git clone https://github.com/tu-usuario/Orquestador_Model_IA.git
cd Orquestador_Model_IA

# Crear entorno virtual
python -m venv venv
venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### Paso 3: Crear Base de Datos (1 minuto)

```powershell
python scripts/create_dummy_db.py
```

**âœ… Salida esperada:**
```
============================================================
âœ“ Base de datos creada exitosamente
âœ“ Total de registros en BD: 1000
============================================================
```

### Paso 4: Configurar Airflow (2 minutos)

```powershell
# Establecer AIRFLOW_HOME
$env:AIRFLOW_HOME="$PWD\airflow"

# Inicializar Airflow
airflow db init

# Crear usuario admin
airflow users create `
    --username admin `
    --firstname Admin `
    --lastname User `
    --role Admin `
    --email admin@example.com `
    --password admin
```

### Paso 5: Iniciar Airflow (1 minuto)

**Terminal 1 - Scheduler:**
```powershell
airflow scheduler
```

**Terminal 2 - Webserver:**
```powershell
airflow webserver --port 8080
```

### Paso 6: Acceder a Airflow

1. Abre tu navegador: **http://localhost:8080**
2. Login:
   - **Usuario:** `admin`
   - **ContraseÃ±a:** `admin`

### Paso 7: Ejecutar el Pipeline

1. Busca el DAG: `etl_sales_pipeline`
2. Activa el toggle (switch a ON)
3. Haz clic en el botÃ³n â–¶ï¸ "Trigger DAG"
4. Â¡Listo! Observa la ejecuciÃ³n en tiempo real

---

## ğŸ³ OpciÃ³n 2: Inicio RÃ¡pido con Docker (Recomendado para Linux/Mac)

### Paso 1: Requisitos Previos

- âœ… Docker instalado
- âœ… Docker Compose instalado

### Paso 2: Clonar el Repositorio

```bash
git clone https://github.com/tu-usuario/Orquestador_Model_IA.git
cd Orquestador_Model_IA
```

### Paso 3: Configurar Variables de Entorno

```bash
cd docker
cp .env.docker .env

# Linux/Mac: Configurar UID
echo "AIRFLOW_UID=$(id -u)" >> .env
```

### Paso 4: Crear Base de Datos (Opcional)

```bash
cd ..
python3 -m venv temp_venv
source temp_venv/bin/activate
pip install pandas python-dotenv
python scripts/create_dummy_db.py
deactivate
```

### Paso 5: Iniciar Servicios

```bash
cd docker
docker-compose up -d
```

**Espera 2-3 minutos** para que los servicios se inicialicen.

### Paso 6: Verificar Estado

```bash
docker-compose ps
```

Todos los servicios deben estar "Up" y "healthy".

### Paso 7: Acceder a Airflow

1. Abre tu navegador: **http://localhost:8080**
2. Login:
   - **Usuario:** `airflow`
   - **ContraseÃ±a:** `airflow`

---

## ğŸ¨ Interfaz de Airflow - GuÃ­a Visual

### Vista Principal (DAGs)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apache Airflow                                    admin â–¼â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DAGs  â”‚  Browse  â”‚  Admin  â”‚  Docs                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚ ğŸ” Search DAGs...                                        â”‚
â”‚                                                          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ etl_sales_pipeline                    âšªâ†’ğŸŸ¢      â”‚   â”‚
â”‚ â”‚ Pipeline ETL para procesar datos de ventas       â”‚   â”‚
â”‚ â”‚ Tags: etl, sales, data-pipeline                  â”‚   â”‚
â”‚ â”‚ Schedule: 0 0 * * * (Daily)                      â”‚   â”‚
â”‚ â”‚ Last Run: Success âœ“                              â”‚   â”‚
â”‚ â”‚ [â–¶ï¸ Trigger] [ğŸ“Š Graph] [ğŸ“‹ Logs]                â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Vista de Grafo (Graph View)

```
start_pipeline
      â†“
extract_data
      â†“
transform_data
      â†“
load_data
      â†“
generate_report
      â†“
end_pipeline
```

**Colores de Estado:**
- ğŸŸ¢ Verde: Ã‰xito
- ğŸ”´ Rojo: Error
- ğŸŸ¡ Amarillo: En ejecuciÃ³n
- âšª Gris: Pendiente

---

## ğŸ“Š Verificar Resultados

### OpciÃ³n A: Consultar la Base de Datos

```powershell
# Abrir SQLite
sqlite3 data/database.db

# Ver datos transformados
SELECT * FROM sales_transformed LIMIT 5;

# Ver estadÃ­sticas
SELECT 
    COUNT(*) as total_registros,
    SUM(total_venta) as ventas_totales,
    AVG(precio_promedio) as precio_promedio
FROM sales_transformed;

# Salir
.quit
```

### OpciÃ³n B: Ver Archivos CSV

```powershell
# Ver datos extraÃ­dos
type data\output\extracted_data.csv | Select-Object -First 10

# Ver datos transformados
type data\output\transformed_data.csv | Select-Object -First 10
```

### OpciÃ³n C: Con Python

```python
import pandas as pd

# Leer datos transformados
df = pd.read_csv('data/output/transformed_data.csv')

# Mostrar primeras filas
print(df.head())

# EstadÃ­sticas bÃ¡sicas
print(df.describe())

# Ventas por regiÃ³n
print(df.groupby('region')['total_venta'].sum().sort_values(ascending=False))
```

---

## ğŸ”§ Comandos Ãštiles

### Airflow CLI

```powershell
# Listar DAGs
airflow dags list

# Ver estado de un DAG
airflow dags state etl_sales_pipeline

# Probar una tarea
airflow tasks test etl_sales_pipeline extract_data 2025-01-01

# Ver logs
airflow tasks logs etl_sales_pipeline extract_data 2025-01-01
```

### Scripts Individuales

```powershell
# Ejecutar solo extracciÃ³n
python scripts/extract_data.py

# Ejecutar solo transformaciÃ³n
python scripts/transform_data.py

# Ejecutar solo carga
python scripts/load_data.py
```

### Docker

```bash
# Ver logs en tiempo real
docker-compose logs -f

# Ver logs de un servicio especÃ­fico
docker-compose logs -f airflow-webserver

# Reiniciar servicios
docker-compose restart

# Detener servicios
docker-compose down

# Eliminar todo (incluyendo datos)
docker-compose down -v
```

---

## â“ SoluciÃ³n de Problemas Comunes

### Problema: "No module named 'airflow'"

**SoluciÃ³n:**
```powershell
# Verificar que el entorno virtual estÃ© activado
venv\Scripts\activate

# Reinstalar Airflow
pip install apache-airflow==2.8.1
```

### Problema: "Port 8080 already in use"

**SoluciÃ³n:**
```powershell
# Cambiar puerto
airflow webserver --port 8081

# O detener el proceso que usa el puerto
netstat -ano | findstr :8080
taskkill /PID <PID> /F
```

### Problema: DAG no aparece en la UI

**SoluciÃ³n:**
```powershell
# Verificar AIRFLOW_HOME
echo $env:AIRFLOW_HOME

# Verificar que el DAG estÃ© en la carpeta correcta
ls dags\

# Verificar errores en el DAG
airflow dags list-import-errors

# Refrescar DAGs (esperar 30 segundos)
```

### Problema: Base de datos no existe

**SoluciÃ³n:**
```powershell
# Crear la base de datos
python scripts/create_dummy_db.py

# Verificar que se creÃ³
ls data\database.db
```

### Problema: Permisos en Docker (Linux/Mac)

**SoluciÃ³n:**
```bash
# Configurar UID correcto
echo "AIRFLOW_UID=$(id -u)" > docker/.env

# Reiniciar servicios
cd docker
docker-compose down
docker-compose up -d
```

---

## ğŸ“š PrÃ³ximos Pasos

Una vez que el pipeline estÃ© funcionando:

1. **Explorar el cÃ³digo:**
   - Lee `dags/etl_pipeline.py` para entender el DAG
   - Revisa `scripts/extract_data.py`, `transform_data.py`, `load_data.py`

2. **Modificar el pipeline:**
   - Cambia la frecuencia de ejecuciÃ³n
   - Agrega nuevas transformaciones
   - Crea nuevas tareas

3. **Experimentar:**
   - Genera mÃ¡s datos dummy
   - Modifica los filtros de extracciÃ³n
   - Agrega nuevas mÃ©tricas

4. **Aprender mÃ¡s:**
   - Lee el [README.md](README.md) completo
   - Consulta la [documentaciÃ³n de Airflow](https://airflow.apache.org/docs/)
   - Revisa [CONTRIBUTING.md](CONTRIBUTING.md) para contribuir

---

## ğŸ‰ Â¡Felicidades!

Has configurado exitosamente un pipeline ETL con Apache Airflow. 

**Â¿Necesitas ayuda?** Abre un [issue en GitHub](https://github.com/tu-usuario/Orquestador_Model_IA/issues)

---

## ğŸ“‹ Checklist de VerificaciÃ³n

- [ ] Python 3.11 instalado
- [ ] Dependencias instaladas (`pip install -r requirements.txt`)
- [ ] Base de datos creada (`python scripts/create_dummy_db.py`)
- [ ] Airflow inicializado (`airflow db init`)
- [ ] Usuario admin creado
- [ ] Scheduler ejecutÃ¡ndose
- [ ] Webserver ejecutÃ¡ndose
- [ ] Acceso a UI (http://localhost:8080)
- [ ] DAG visible en la UI
- [ ] DAG activado
- [ ] Pipeline ejecutado exitosamente
- [ ] Resultados verificados

**Â¡Todo listo! ğŸš€**
