# ============================================================================
# CONFIGURACIÓN DE APACHE AIRFLOW
# ============================================================================
# Este archivo contiene la configuración básica para Apache Airflow.
# Para una configuración completa, ejecuta: airflow db init
# Esto generará un archivo airflow.cfg completo en AIRFLOW_HOME
# ============================================================================

[core]
# Carpeta donde se encuentran los DAGs
dags_folder = d:/GitHub/Orquestador_Model_IA/dags

# Carpeta base de Airflow
airflow_home = d:/GitHub/Orquestador_Model_IA/airflow

# Executor a utilizar (LocalExecutor, SequentialExecutor, CeleryExecutor)
executor = LocalExecutor

# No cargar DAGs de ejemplo
load_examples = False

# Nivel de paralelismo
parallelism = 32

# Número máximo de DAGs activos
max_active_runs_per_dag = 1

# Zona horaria
default_timezone = UTC

[database]
# Conexión a la base de datos de metadatos de Airflow
# Para SQLite (desarrollo):
sql_alchemy_conn = sqlite:///d:/GitHub/Orquestador_Model_IA/airflow/airflow.db

# Para PostgreSQL (producción):
# sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost/airflow

[webserver]
# Puerto del servidor web
web_server_port = 8080

# Host del servidor web
web_server_host = 0.0.0.0

# Exponer configuración en la UI
expose_config = True

# Habilitar RBAC
rbac = True

# Tiempo de sesión (en segundos)
session_lifetime_minutes = 43200

[scheduler]
# Intervalo de actualización de DAGs (en segundos)
dag_dir_list_interval = 30

# Número de procesos de parsing
parsing_processes = 2

# Habilitar health check
enable_health_check = True

[logging]
# Nivel de logging
logging_level = INFO

# Nivel de logging para FAB (Flask App Builder)
fab_logging_level = WARN

# Carpeta de logs
base_log_folder = d:/GitHub/Orquestador_Model_IA/airflow/logs

# Formato de logs
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

[api]
# Habilitar autenticación en la API
auth_backends = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session

[operators]
# Tiempo de espera por defecto para sensores
default_owner = airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0

[smtp]
# Configuración de email (opcional)
# smtp_host = smtp.gmail.com
# smtp_starttls = True
# smtp_ssl = False
# smtp_user = your-email@gmail.com
# smtp_password = your-password
# smtp_port = 587
# smtp_mail_from = airflow@example.com

[celery]
# Configuración de Celery (solo si usas CeleryExecutor)
# broker_url = redis://localhost:6379/0
# result_backend = db+postgresql://airflow:airflow@localhost/airflow

[secrets]
# Backend de secretos (opcional)
# backend = airflow.providers.hashicorp.secrets.vault.VaultBackend
# backend_kwargs = {"connections_path": "connections", "variables_path": "variables", "url": "http://127.0.0.1:8200"}

# ============================================================================
# NOTAS IMPORTANTES:
# ============================================================================
# 1. Este es un archivo de configuración básico para desarrollo
# 2. Para producción, considera usar PostgreSQL en lugar de SQLite
# 3. Ajusta las rutas según tu sistema operativo
# 4. Para más opciones, consulta: https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html
# ============================================================================
